{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project I - Image Classification\n",
    "\n",
    "**Team**: Filip Ko≈Çodziejczyk, Jerzy Kraszewski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this project is to create a model that can classify images of 10 different classes. The dataset used for this project is the CINIC-10 dataset, which is a combination of CIFAR-10 and ImageNet. The dataset contains 270,000 images, which are divided into 10 classes of equal size. The classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The images are 32x32 pixels in size and are in RGB format. Data is divided into training, validation, and test sets, equally for each class.\n",
    "More details about the dataset can be found [here](https://datashare.ed.ac.uk/handle/10283/3192) and [here](https://www.kaggle.com/datasets/mengcius/cinic10/data).\n",
    "\n",
    "TODO: Add citation for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "We load all the necessary libraries and set an appropriate backend for the PyTorch for most optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import models, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import timm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and adjusting data split\n",
    "\n",
    "The original dataset has predefined split of data. We adjust it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train size</th>\n",
       "      <th>test size</th>\n",
       "      <th>valid size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truck</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bird</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airplane</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ship</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frog</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horse</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deer</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automobile</th>\n",
       "      <td>18900</td>\n",
       "      <td>4050</td>\n",
       "      <td>4050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train size  test size  valid size\n",
       "cat              18900       4050        4050\n",
       "dog              18900       4050        4050\n",
       "truck            18900       4050        4050\n",
       "bird             18900       4050        4050\n",
       "airplane         18900       4050        4050\n",
       "ship             18900       4050        4050\n",
       "frog             18900       4050        4050\n",
       "horse            18900       4050        4050\n",
       "deer             18900       4050        4050\n",
       "automobile       18900       4050        4050"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset must be downloaded from the link provided in Introduction and put into `data` directory.\n",
    "# It should be renamed to `cinic10.zip`.\n",
    "\n",
    "archive_path = \"data/cinic10.zip\"\n",
    "data_dir = \"data/cinic10\"\n",
    "data_subdirs = [\"train\", \"test\", \"valid\"]\n",
    "props = [0.7, 0.15, 0.15]\n",
    "\n",
    "if sum(props) != 1:\n",
    "    raise ValueError(\"Props must sum to 1\")\n",
    "\n",
    "# Extracting the data\n",
    "with ZipFile(archive_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "classes = os.listdir(os.path.join(data_dir, \"train\"))\n",
    "\n",
    "# Changing the data split\n",
    "for cls in classes:\n",
    "    dirs = [os.path.join(data_dir, subdir, cls) for subdir in data_subdirs]\n",
    "    sizes = [len(os.listdir(d)) for d in dirs]\n",
    "    total = sum(sizes)\n",
    "    target_sizes = [int(p * total) for p in props]\n",
    "    diffs = [target_sizes[i] - sizes[i] for i in range(len(sizes))]\n",
    "\n",
    "    for i in range(len(diffs)):\n",
    "        if diffs[i] < 0:\n",
    "            for j in range(len(diffs)):\n",
    "                if diffs[j] > 0:\n",
    "                    count = min(abs(diffs[i]), diffs[j])\n",
    "                    files = os.listdir(dirs[i])\n",
    "                    files = files[:count]\n",
    "                    for f in files:\n",
    "                        shutil.move(os.path.join(dirs[i], f), os.path.join(dirs[j], f))\n",
    "                    diffs[i] += count\n",
    "                    diffs[j] -= count\n",
    "\n",
    "# Checking the sizes\n",
    "cls_sizes = {}\n",
    "for cls in classes:\n",
    "    cls_sizes[cls] = [len(os.listdir(os.path.join(data_dir, subdir, cls))) for subdir in data_subdirs]\n",
    "pd.DataFrame.from_dict(cls_sizes, orient='index', columns=[f\"{set} size\" for set in data_subdirs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and augumenting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10000\n",
      "Test size: 1000\n",
      "Valid size: 1000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# No augumentation for now except default for ResNet\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transforms)\n",
    "test = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=transforms)\n",
    "valid = datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=transforms)\n",
    "\n",
    "# Taking a subsets for rapid prototyping\n",
    "train_subset_indices = torch.randperm(len(train))[:10000]\n",
    "test_subset_indices = torch.randperm(len(test))[:1000]\n",
    "valid_subset_indices = torch.randperm(len(valid))[:1000]\n",
    "\n",
    "train = Subset(train, train_subset_indices)\n",
    "test = Subset(test, test_subset_indices)\n",
    "valid = Subset(valid, valid_subset_indices)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train size: {len(train)}\")\n",
    "print(f\"Test size: {len(test)}\")\n",
    "print(f\"Valid size: {len(valid)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet18\", pretrained=True, num_classes=len(classes))\n",
    "model = model.to(device)\n",
    "optim = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        batch_count += 1\n",
    "\n",
    "        optim.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader.dataset)\n",
    "    valid_acc = correct / total\n",
    "\n",
    "    print(f'Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8158\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {correct / total:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
